{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports classiques\n",
    "Nous allons tout d'abord lancer les imports classiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Optim_corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9a1080af69a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mOptim_corr\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions_corr\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Optim_corr'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functions as func\n",
    "import Optim_corr as opt\n",
    "import functions_corr as func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modification des algorithmes\n",
    "Nous allons modifier la structure des algorithmes d'optimisation et les données qu'ils rendent. Au lieu de rendre plusieurs listes `x_list,cost_list,grad_list,...`, les algorithmes vont rendre un seul résultat `to_return` qui est un dictionnaire qui aura plusieurs entrées. Voici une liste de commandes pour comprendre le fonctionnement d'un dictionnaire "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copiez votre fichier dans un fichier `Optim.py` dans un fichier `Optim2.py`. Et modifiez toutes vos fonctions pour quelles vous rendent juste un dictionnaire.\n",
    "Nous allons maintenant créer une fonction `Wolfe(J,x,d,cost,grad,s=1.,itermax=20,e1=1.e-4,e2=0.99)` qui remplace la recherche de pas de Wolfe si on va dans la direction de descente `d`. les variables `cost` et `grad` sont respectivement la valeur de `J(x)` et le gradient $\\nabla J(x)$ Je l'ai mise dans un fichier `LibOptim.py`. Cette fonction devra vous rendre `s,cost_new,grad_new` qui sont respectivement : le pas trouvé par la méthode de Wolfe, le coût au nouveau point et le gradient au nouveau point, c'est à dire, respectivement $J(x+sd)$ et $\\nabla J(x+sd)$.\n",
    "Modifiez en conséquence votre fonction `Optim2.py` et vérifiez que vous n'avez rien cassé en lançant le genre de commandes suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) 0.0\n"
     ]
    }
   ],
   "source": [
    "import LibOptim as lib\n",
    "import Optim as opt1\n",
    "import Optim2 as opt\n",
    "\n",
    "f=func.Rosen()\n",
    "x0=np.array([-1,0])\n",
    "x_list,cost_list,grad_list,step_list = opt1.wolfe(f,x0,itermax=10000)\n",
    "r = opt.wolfe(f,x0,itermax=10000)\n",
    "diff = np.array(cost_list)-np.array(r['cost_list'])\n",
    "print(diff.shape, np.linalg.norm(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de Newton\n",
    "Nous nous intéressons à la méthode de Newton, il s'agit ici de choisir une direction de descente\n",
    "$$d_k=H[J](v_k)^{-1}\\nabla J(v_k)$$\n",
    "et un pas de $1$\n",
    "Créez une fonction `Newton(J,x_0,itermax=500,tol=1.e-5)` dans le fichier `Optim2.py` et lancez la méthode de Newton sur les différentes fonctions tests. Cette méthode vous un dictionnaire avec les entrées `x_list,cost_list,grad_list,step_list`, voir le TP1 pour les explications sur les différentes variables.\n",
    "Vous rajouterez aussi dans le dictionnaire l'entrée `angle_list` qui est le cosinus de l'angle fait entre la direction de descente choisie par l'algorithme et le gradient. On vérifiera que `angle_list` est négatif et proche de `1`. \n",
    "Pour faire l'inversion du système, on utilisera la fonction `solve` de la bibliothèque `scipy.linalg`.\n",
    "Testez ci-dessous votre algorithme pour les différentes fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_list': [array([-1,  0]), array([-0.99004975,  0.9800995 ]), array([ 0.9613592 , -2.88378537]), array([0.96140987, 0.92430893]), array([0.99999998, 0.99851076]), array([1.        , 0.99999999]), array([1., 1.])], 'cost_list': [3.960298992672956, 1450.0855138980314, 0.0014891985191022043, 0.0002217707674237523, 2.06791962366025e-17, 0.0], 'grad_list': [450.7948535642349, 4.019357199736802, 1650.4854469527404, 0.0771792826529715, 0.6659890941821408, 9.094788033900982e-09], 'step_list': [1, 1, 1, 1, 1, 1], 'angle_list': [0.8992537197422174, 0.9020051331828127, 0.8954177090313139, 0.8954268869086202, 0.9016595473673641, 0.9016537095157616]}\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "f = func.Rosen()\n",
    "x0 = np.array([-1, 0])\n",
    "r = opt.Newton(f, x0, itermax=500, tol=1.e-5)\n",
    "print(r)\n",
    "print(f.fcounter)\n",
    "print(f.gcounter)\n",
    "print(f.hcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-1,  0]),\n",
       " array([-0.99004975,  0.9800995 ]),\n",
       " array([ 0.9613592 , -2.88378537]),\n",
       " array([0.96140987, 0.92430893]),\n",
       " array([0.99999998, 0.99851076]),\n",
       " array([1.        , 0.99999999]),\n",
       " array([1., 1.])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[\"x_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stabilisation de Newton\n",
    "Normalement vous avez vu que l'algorithme de Newton ne fonctionne pas très bien avec la fonction `oscill`. L'objectif est de stabiliser cet algorithme de Newton en y rajoutant une recherche de pas de style `Wolfe`. Implémentez une foncion `Newton_Wolfe` qui concatène les algorithmes de `Newton` et de `Wolfe`. Vous prendrez bien garde à regarder le nombre de calculs que vous faites de la Hessienne, du gradient et d'évaluations de la fonction.\n",
    "De plus l'algorithme de Wolfe ne va pas très bien fonctionné si la direction de choisie n'est pas de descente, pour cela si vous trouvez que $(d,\\nabla J(x)) >0$, vous prendrez comme direction de descente la direction donnée par $d=-\\nabla J(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "507\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "f = func.oscill()\n",
    "x0 = np.array([-2, -2])\n",
    "r = opt.Newton_Wolfe(f, x0, itermax=500, tol=1.e-5)\n",
    "#print(r)\n",
    "print(f.fcounter)\n",
    "print(f.gcounter)\n",
    "print(f.hcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_list': [array([-2., -2.]),\n",
       "  array([-2.00012207, -2.00012207]),\n",
       "  array([-2.00011782, -2.0002277 ]),\n",
       "  array([-2.00026531, -2.00033869]),\n",
       "  array([-2.0003755 , -2.00049965]),\n",
       "  array([-2.0003755 , -2.00049965]),\n",
       "  array([nan, nan]),\n",
       "  array([nan, nan])],\n",
       " 'cost_list': [array([2.83281062, 2.83281062]),\n",
       "  array([2.83299244, 2.83325796]),\n",
       "  array([2.83355071, 2.83372805]),\n",
       "  array([2.83410972, 2.83440979]),\n",
       "  array([2.83410972, 2.83440979]),\n",
       "  array([nan, nan]),\n",
       "  array([nan, nan])],\n",
       " 'grad_list': [3.0240788312284432,\n",
       "  3.0242706872545706,\n",
       "  3.024288814976173,\n",
       "  3.0245123401739584,\n",
       "  3.0246969139940965,\n",
       "  3.0246969139940965,\n",
       "  nan],\n",
       " 'step_list': [6.103515625e-05,\n",
       "  6.103515625e-05,\n",
       "  6.103515625e-05,\n",
       "  6.103515625e-05,\n",
       "  4.81482486096809e-35,\n",
       "  4.81482486096809e-35,\n",
       "  4.81482486096809e-35],\n",
       " 'angle_list': [0.5485304671885517,\n",
       "  0.8425915142414898,\n",
       "  0.5403023058681395,\n",
       "  0.5836937224051124,\n",
       "  0.6513291448813762,\n",
       "  nan,\n",
       "  nan]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS\n",
    "Nous allons nous intéresser à l'algorithme Limited Memory BFGS. Cet algorithme est du type BFGS, c'est à dire qu'il estime l'inverse de la Hessienne de `J`. Le `L` dans l'algorithme signifie qu'il est à mémoire limitée, c'est à dire qu'il ne garde en mémoire que les $L$ dernières itérations de calcul pour estimer la Hessienne.\n",
    "L'algorithme est le suivant : Nous sommes à l'itération $k$, nous notons $x_k$ l'itéré et nous avons stocké les vecteurs suivants pour tout $k_{min}\\le i\\le k$.\n",
    "$$ s_i=x_{i}-x_{i-1} \\text{ et } y_i=\\nabla J(x_{i}) -\\nabla J(x_{i-1})$$\n",
    "Et on a aussi stocké $\\rho_i=\\frac{1}{(s_i,y_i)}$. Tous les $\\rho_i$ doivent être positifs.\n",
    "L'algorithme est le suivant \n",
    "# Algorithme\n",
    "\n",
    "$r=-\\nabla J(x_k)$\n",
    "\n",
    "Pour $i=k,k-1,\\dots k_{min}$\n",
    "\n",
    "... $\\alpha_i=\\rho_i(s_i \\cdot r)$\n",
    "\n",
    "... $r=r-\\alpha_i y_i$\n",
    "\n",
    "$r=\\displaystyle \\frac{(s_k\\cdot y_k)}{(y_k\\cdot y_k)}*r$\n",
    "\n",
    "Pour $i=k_{min},k_{min}+1,\\dots,k$\n",
    "\n",
    "... $\\beta_i=\\rho_i(y_i\\cdot r)$\n",
    "\n",
    "... $r=r+(\\alpha_i-\\beta_i)s_i$\n",
    "\n",
    "rend r\n",
    "# Fonctions sur les listes\n",
    "Vous aurez sans doute besoin des fonctions suivantes pour les listes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (2, 3), (4, 6), (6, 9), (8, 12)]\n",
      "(6, 9) (8, 12)\n",
      "*** Pop ***\n",
      "[(2, 3), (4, 6), (6, 9), (8, 12)]\n",
      "*** Parcours ***\n",
      "2 et 3\n",
      "4 et 6\n",
      "6 et 9\n",
      "8 et 12\n",
      "*** Parcours Inverse***\n",
      "(8, 12)\n",
      "(6, 9)\n",
      "(4, 6)\n",
      "(2, 3)\n",
      "[4, 16, 36, 64]\n",
      "*** Parcours de deux listes ensembles***\n",
      "4 et 2 et encore 3\n",
      "16 et 4 et encore 6\n",
      "36 et 6 et encore 9\n",
      "64 et 8 et encore 12\n",
      "*** Append ***\n",
      "[4, 16, 36, 64]\n",
      "[4, 16, 36, 64, 546]\n",
      "*** Inversion ***\n",
      "[546, 64, 36, 16, 4]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a=[(2*i,3*i) for i in range(5) ]\n",
    "print(a)\n",
    "print(a[3],a[-1])\n",
    "print('*** Pop ***')\n",
    "a.pop(0)\n",
    "print(a)\n",
    "print('*** Parcours ***')\n",
    "for e,f in a :\n",
    "      print(e,'et',f)\n",
    "print('*** Parcours Inverse***')\n",
    "for e in reversed(a) :\n",
    "      print(e)\n",
    "b=[e**2 for e,f in a]\n",
    "print(b)\n",
    "print('*** Parcours de deux listes ensembles***')\n",
    "for (m,(t,p)) in zip(b,a) :\n",
    "    print(m,'et',t,'et encore',p)\n",
    "print('*** Append ***')\n",
    "print(b)\n",
    "b.append(546)\n",
    "print(b)\n",
    "print('*** Inversion ***')\n",
    "c=list(reversed(b))\n",
    "print(c)\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class BFGS\n",
    "Créez une classe `BFGS` dans `LibOptim.py`, sa fonction `__init__` sera de la forme \n",
    "`__init__(self,nb_stock_max=8)` où `nb_stock_max` est le nombre maximum d'itérations prises en compte. Cette fonction créera aussi une liste vide appelée `stock` qui conserve les $s_i,g_i,\\rho_i$. Elle dera aussi créer une liste vide nommée `last_iter`.\n",
    "\n",
    "# Push\n",
    "Nous allons maintenant créer une fonction `push(self, x, grad)` qui enregistre $s_k,g_k,\\rho_k$. Pour cela, on a besoin de $x_{k-1},\\nabla J(x_{k-1})$. Si ils existent, ils se trouvent dans la liste `self.last_iter`. Ensuite on peut calculer $s_k,g_k$ et $\\rho_k$. \n",
    "\n",
    "Si $\\rho_k$ est positif, alors on enregistre le triplet $(s_k,g_k,\\rho_k)$ à la fin de la liste `self.stock`, en vérifiant que stock ne doit contenir au maximum que les dernières `self.nb_stock_max` itérations. \n",
    "\n",
    "Si $\\rho_k$ est négatif, quelquechose c'est mal passé, on vide le `self.stock`.\n",
    "\n",
    "A la fin, on n'oublie pas de mettre $x_{k}$ et $\\nabla J(x_{k})$ dans `self.last_iter` pour être sûr de les y trouver la prochaine fois.\n",
    "\n",
    "# Get\n",
    "\n",
    "Nous allons maintenant créer une fonction `get(self, grad)` qui modifie la direction de descente et applique l'algorithme ci-dessus. Cette fonction doit nous rendre le `r` final. Si le `self.stock` est vide, cette fonction doit nous rendre `-grad`\n",
    "\n",
    "# C'est l'heure de tester ...\n",
    "Lancez une méthode de Newton_Wolfe sur votre fonction préférée et à chaque itération calculez ce que donnerait un L-BFGS. Comparez les angles des directions entre la méthode de Newton et le L-BFGS, comparez aussi le ration des normes. Ensuite lancez un LBFGS avec recherche de pas de Wolfe sur vos tests préférez et obtenez le comportement de Newton_Wolfe sans le calcul de la Hessienne...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning\n",
      "warning\n",
      "476\n",
      "134\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julienblanchon/Git/julien-blanchon/Le-TP-de-Keunotor3/optimisation/tp/tp2/LibOptim.py:17: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  rhok = 1.0/(np.dot(sk, yk))\n",
      "/Users/julienblanchon/Git/julien-blanchon/Le-TP-de-Keunotor3/optimisation/tp/tp2/LibOptim.py:35: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  alpha.append(rhoi*np.dot(si,r))\n",
      "/Users/julienblanchon/Git/julien-blanchon/Le-TP-de-Keunotor3/optimisation/tp/tp2/LibOptim.py:38: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  r = r*(np.dot(sk, yk)/np.dot(yk, yk))\n"
     ]
    }
   ],
   "source": [
    "f = func.oscill()\n",
    "x0 = np.array([-2., -2.])\n",
    "r = lib.optim_bfgs(f, x0, itermax=500, tol=1.e-5)\n",
    "#print(r)\n",
    "print(f.fcounter)\n",
    "print(f.gcounter)\n",
    "print(f.hcounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_list': [array([-2., -2.]),\n",
       "  array([-2.00012207, -2.00012207]),\n",
       "  array([-2.00011782, -2.0002277 ]),\n",
       "  array([-2.00026531, -2.00033869]),\n",
       "  array([-2.0003755 , -2.00049965]),\n",
       "  array([-2.0003755 , -2.00049965]),\n",
       "  array([nan, nan]),\n",
       "  array([nan, nan])],\n",
       " 'cost_list': [array([2.83281062, 2.83281062]),\n",
       "  array([2.83299244, 2.83325796]),\n",
       "  array([2.83355071, 2.83372805]),\n",
       "  array([2.83410972, 2.83440979]),\n",
       "  array([2.83410972, 2.83440979]),\n",
       "  array([nan, nan]),\n",
       "  array([nan, nan])],\n",
       " 'grad_list': [3.0240788312284432,\n",
       "  3.0242706872545706,\n",
       "  3.024288814976173,\n",
       "  3.0245123401739584,\n",
       "  3.0246969139940965,\n",
       "  3.0246969139940965,\n",
       "  nan],\n",
       " 'step_list': [6.103515625e-05,\n",
       "  6.103515625e-05,\n",
       "  6.103515625e-05,\n",
       "  6.103515625e-05,\n",
       "  4.81482486096809e-35,\n",
       "  4.81482486096809e-35,\n",
       "  4.81482486096809e-35],\n",
       " 'angle_list': [0.5485304671885517,\n",
       "  0.8425915142414898,\n",
       "  0.5403023058681395,\n",
       "  0.5836937224051124,\n",
       "  0.6513291448813762,\n",
       "  nan,\n",
       "  nan]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
