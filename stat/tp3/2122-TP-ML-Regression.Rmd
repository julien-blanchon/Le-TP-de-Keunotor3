---
title: "TP modèles linéaires - Partie 1"
date : "4modIA - 2021-2022"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth : 4
    number_sections : true
editor_options: 
  markdown: 
    wrap: 72
---

```{css,echo=F}
.badCode {
background-color: #C9DDE4;
}
```

```{r setup, echo=FALSE, cache=FALSE}
library(knitr)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               class.source="badCode")
opts_knit$set(width=75)
```

L'objectif de ce TP est d'illustrer les notions abordées dans le
chapitre de régression linéaire.

Les librairies R nécessaires pour ce TP :

```{r,echo=T, error=F,warning=F}
library(ellipse)
library(leaps)
library(MASS)
library(corrplot)
library(glmnet)
library(coefplot)
library(ggplot2)  # A mettre en commentaire si non utilisation des graphes ggplot
library(gridExtra)# A mettre en commentaire si non utilisation des graphes ggplot
library(ggfortify)# A mettre en commentaire si non utilisation des graphes ggplot
library(plotly)   # A mettre en commentaire si non utilisation des graphes ggplot
library(reshape2)
```

# Introduction

La pollution de l'air constitue actuellement une des préoccupations
majeures de santé publique. De nombreuses études épidémiologiques ont
permis de mettre en évidence l'influence sur la santé de certains
composés chimiques comme le dioxyde souffre (SO2), le dioxyde d'azote
(NO2), l'ozone (O3) ou des particules en suspension. Des associations de
surveillance de la qualité de l'air (Air Breizh en Bretagne depuis 1994)
existent sur tout le territoire métropolitain et mesurent la
concentration des polluants. Elles enregistrent également les conditions
météorologiques comme la température, la nébulosité, le vent, les chutes
de pluie en relation avec les services de Météo France... L'une des
missions de ces associations est de construire des modèles de prévision
de la concentration en ozone du lendemain à partir des données
disponibles du jour : observations et prévisions de Météo France. Plus
précisément, il s'agit d'anticiper l'occurrence ou non d'un dépassement
légal du pic d'ozone (180 $\mu$gr/m3) le lendemain afin d'aider les
services préfectoraux à prendre les décisions nécessaires de prévention
: confinement des personnes à risque, limitation du trafic routier. Plus
modestement, l'objectif de cette étude est de mettre en évidence
l'influence de certains paramètres sur la concentration d'ozone (en
$\mu$gr/m3) et différentes variables observées ou leur prévision. Les
112 données étudiées ont été recueillies à Rennes durant l'été 2001.

Les 13 variables observées sont :

-   maxO3 : Maximum de concentration d'ozone observé sur la journée en
    $\mu$gr/m3
-   T9, T12, T15 : Température observée à 9, 12 et 15h
-   Ne9, Ne12, Ne15 : Nébulosité observée à 9, 12 et 15h
-   Vx9, Vx12, Vx15 : Composante E-O du vent à 9, 12 et 15h
-   maxO3v : Teneur maximum en ozone observée la veille
-   vent : orientation du vent à 12h
-   pluie : occurrence ou non de précipitations

Les données sont disponibles sur la page moodle du cours. Pour les
importer, vous pouvez utiliser la commande suivante :

```{r}
ozone = read.table("Ozone.txt")
```

Afin de vous familiariser avec les données, faites dans un premier temps
une analyse de statistique descriptive. Vous pouvez utiliser les
fonctions `summary()`, `boxplot()`, `pairs()`, `barplot()`,
`corrplot()`, ....

```{r}
ozone$vent = as.factor(ozone$vent) #IMPORTANTTTT
ozone$pluie = as.factor(ozone$pluie)
summary
```
**Important de transformer les variable qualitative.**

```{r}
boxplot(ozone)
```
**Les variables maxO3 et ? on une bien plus grande variance que les autres. Il y a de fort outlier.**

```{r}
pairs(ozone[,1:5])
```
**Certaines variables semble avoir un corrélation linéaire**

```{r}
pairs(ozone)
```
```{r}
corrplot(cor(ozone[, 1:11]), method='ellipse')
```
```{r}
ggplot(melt(ozone[, 1:11]), aes(x=variable, y=value)) + geom_violin()
```
```{r}
barplot(table(ozone$vent),main="Effectifs")
barplot(table(ozone$pluie),main="Effectifs")
```
e(ozone)

# Régression linéaire simple

Dans cette section, nous souhaitons expliquer la concentration d'ozone
maximale de la journée (maxO3) en fonction de la température à 12h
(T12).

1.  Représentez le nuage de points $(x_i,y_i)$ à l'aide de la fonction
    `plot()` (ou `geom_point()` de *ggplot2*).

```{r}
plot(x=ozone$maxO3, y=ozone$T12)
```

<!--  version ggplot -->

```{r,echo=F,eval=F}
ggplot(ozone,aes(x=T12,y=maxO3))+
  geom_point()
```

Question : Pensez-vous que l'ajustement d'un modèle de régression
linéaire $y_i = \theta_0+\theta_1 x_i +\varepsilon_i$ est justifié
graphiquement? 

Réponse : Oui x et y semble linéairement corrélé

2.  Effectuez la régression linéaire à l'aide de la fonction `lm()` et
    exploitez les résultats.

```{r, eval=F}
reg.simple = lm(formula=maxO3~T12, data = ozone)  # A COMPLETER
summary(reg.simple)
plot(maxO3~T12, data = ozone, pch=20)
abline(reg.simple, col="red")
```

Interprétation : La pval de la régression est plutôt faible mais le $R^2$ est plutôt faible une corrélation linéaire semble possible mais la variable explicative T12 ne suffit pas pour expliquer max03. La pval de la nullité de T12 est trés faible.

17.57 --> sigma chap carré
110 --> n-k (n=112 et k=2)
F-statistic --> Test de Fisher de sous modèle entre (M1) reg linéaire et (M0) avec seulement intercept.

Que contiennent les sorties suivantes :

```{r, eval=F}
reg.simple$coefficients  
reg.simple$residuals
```

Réponse : 
* -27.419636    5.468685 sont les coefs $\theta_1$ et $\theta_2$ du modèle de régression
* Les autres coefs sont les $\hat{\epsilon^_i}$.

3.  A l'aide des commandes suivantes, tracez l'estimation de la droite
    de régression sur le nuage de points ainsi qu'un intervalle de
    confiance à $95\%$ de celle-ci :

```{r}
ggplot(ozone, aes(T12, maxO3))+
    geom_point() +
    geom_smooth(method=lm, se=TRUE)+
    xlab("T12")+  ylab("maxO3")
```

<!--  Version plot -->

```{r,echo=F,eval=F}
plot(maxO3~T12,data=ozone,pch=20)
abline(reg.simple)
T12=seq(min(ozone[,"T12"]),max(ozone[,"T12"]),
 length=100)
grillex<-data.frame(T12)
ICdte<-predict(reg.simple,new=grillex,
              interval="confidence",level=0.95)
matlines(grillex$T12,cbind(ICdte),lty=c(1,2,2),
                           col="red")
```

Ce graphique permet visuellement de vérifier l'ajustement des données au
modèle de régression linéaire. Que remarquez-vous?

4.  A l'aide de la commande suivante, étudiez les résidus

```{r,eval=F}
autoplot(reg.simple,which=c(1,2,4),label.size=2)     
```
Interprétation :
* Sur le Res/Fit il semblerais que l'on est bien $E[\epsilon_i] = 0$ (H1) car centrée sur 0, $Var[\epsilon_i] \sigma^2$ (H2) car les $\epsilon_i^2$ sont dans une bande.Les $\epsilon_i$ semble indépendant (H3) car un peu aléatoire.

* Sur le QQplot il semblerais qu'il n'y est pas de queue de distribution donc on peut supposé que les $\epsilon_i$  suive  une loi normale (H4).

On prendra soin de comprendre les différents graphiques obtenus.

5.  On s'intéresse maintenant à la qualité de prédiction du modèle. On
    va donc tracer un intervalle de confiance des prédictions à $95\%$
    avec les commandes suivantes. Commentez.

```{r,eval=F}
temp_var = predict(reg.simple, 
                  interval="prediction")
new_df = cbind(ozone, temp_var)
ggplot(new_df, aes(T12, maxO3))+
    geom_point() +
    geom_line(aes(y=lwr), color = "red", 
                       linetype = "dashed")+
    geom_line(aes(y=upr), color = "red", 
                       linetype = "dashed")+
    geom_smooth(method=lm, se=TRUE)+
    xlab("T12")+  
    ylab("maxO3")
```

<!--  version en plot -->

```{r,eval=F,echo=F}
plot(maxO3~T12,data=ozone,pch=20)
ICprev<-predict(reg.simple,new=grillex,
                   interval="pred",level=0.95)
matlines(grillex$T12,cbind(ICprev),lty=c(1,2,2),
                          col="blue")
```

6.  On va maintenant s'intéresser à la construction d'intervalles de
    confiance pour $\theta_0$ et $\theta_1$ à $95\%$.\
    A l'aide de la fonction `confint()`, construisez un intervalle de
    confiance pour chaque paramètre séparément.

```{r}
IC = confint(object = reg.simple, level = 0.95)
```

Pour tenir compte de la dépendance entre $\theta_0$ et $\theta_1$, on
peut aussi construire une région de confiance pour le vecteur
$\theta=(\theta_0,\theta_1)'$ grâce aux commandes suivantes. Comparez
les résultats.

```{r, eval=F}
df1 = as.data.frame(
             rbind(coefficients(reg.simple),
             ellipse(reg.simple,level=0.95)))
colnames(df1) = c("intp", "slope")
ggplot(data=df1[-1,],aes(x=intp, y=slope))+
  geom_path()+
  annotate("rect",xmin=IC[1,1],xmax=IC[1,2],
  ymin=IC[2,1],ymax=IC[2,2],fill="red",alpha=0.1)+
  geom_point(data=df1[1,], aes(x=intp, y=slope), 
               pch=3)
```

# Régression linéaire multiple

Dans cette partie, nous souhaitons analyser la relation entre le maximum
journalier de la concentration d'ozone (maxO3) et

-   la température à différentes heures de la journée (T9, T12, T15),
-   la nébulosité à différentes heures de la journée (Ne9, Ne12, Ne15),
-   la projection du vent sur l'axe Est-Ouest à différentes heures de la
    journée (Vx9, Vx12,Vx15),
-   la concentration maximale d'ozone de la veille (maxO3v)

On va donc utiliser le sous-jeu de données suivant

```{r}
ozone1 = ozone[,1:11]
```

et mettre en place un modèle de régression linéaire multiple.

1.  Faites une analyse descriptive de ce jeu de données.

```{r}
corrplot(cor(ozone1), method='ellipse')
ggplot(melt(ozone1), aes(x=variable, y=value)) + geom_violin()
```

2.  Rappelez l'écriture mathématique du modèle de régression linéaire
    multiple.

$$
y_i = \theta_0 + \theta_1x_1 + \cdots  + \theta_px_p + \epsilon_i
$$

3.  Ajustez un modèle de régression linéaire multiple à l'aide de la
    commande `lm()` et commentez les résultats (on appelle reg.mul la
    sortie de R dans la suite).

```{r,eval=F}
reg.mul = lm(formula = maxO3~., data=ozone) # A COMPLETER
resume.mul = summary(reg.mul)
summary(reg.mul)
```

4.  Etudiez les résidus

```{r}
reg.mul$coefficients  
reg.mul$residuals
summary(reg.mul$residuals)
autoplot(reg.mul, which=1:2, label.size=2)     
```

# Sélection de variables

Dans le `summary(reg.mul)`, un test est fait sur chaque coefficient. Il
revient à tester que la variable n'apporte pas d'information
supplémentaire sachant que toutes les autres variables sont dans le
modèle. Il est donc préférable d'utiliser des procédures de choix de
modèles pour sélectionner les variables significatives. On va ici
comparer la sélection de variable obtenue par différents critères: BIC,
AIC, $R^2$ ajusté, Cp de Mallows. Pour cela, vous pouvez utiliser la
fonction `regsubsets()` de la librairie *leaps* et la fonction
`stepAIC()` de la librairie *MASS*. Commentez les résultats obtenus avec
les différents critères, vous pourrez vous aider des commandes suivantes
:

```{r,eval=F}
choix_cp = regsubsets(maxO3~., data=ozone1, nbest=1, nvmax=10, method="backward")
summary(choix_cp)
plot(choix_cp, scale="Cp")

choix_bic = regsubsets(maxO3~., data=ozone1, nbest=1 ,nvmax=10, method="backward")
plot(choix_bic, scale="bic")
```
Avec le critère BIC, nous retenons 4 variables : T12, Ne9, Vx9 et
maxO3v.
A l'aide des commandes suivantes, testez le sous-modèle avec les 4
variables retenues par BIC contre le modèle complet. 

On remarque que BIC a selectionner une variable dans chaque block (T, N, V). Ce qui est logique à la vue du corrplot 

```{r}
choix_aic = stepAIC(reg.mul)
```

À chaque étape stepAIC essaye d'enlever une variable (- lenomvariable). Il supprime en 1er le sous modele avec l'AIC la plus faible. Il s'arrete quand le meilleurs sous modele est d'enlever none.


```{r,eval=F}
reg.fin = lm(maxO3~T12+Ne9+Vx9+maxO3v, data=ozone1)
summary(reg.fin)
anova(reg.fin, reg.mul)
```

L'anova nous fait un test de Fisher de sous model avec un un pval de 0.987 (zone de rejet trés faible), on ne rejette pas H0. Le model 1 est acceptés. On peut comparer les R2 ajusté.

RSS = 20970 somme des résidus dans le modele simplifié SSR1
RSS - 20410 somme des résidus dans le modele simplifié SSR0

# Régressions régularisées

On commence par centrer et réduire les données avant de mettre en place
et comparer des méthodes de régression régularisées.

```{r}
tildeY=scale(ozone1[,1],center=T,scale=T)
tildeX=scale(ozone1[,-1],center=T,scale=T)
```

## Régression Ridge

1.  A l'aide de la fonction `glmnet()`, ajustez une régression ridge en
    faisant varier $\lambda$ sur une grille. On stockera le résultat
    dans la variable `fitridge`. Explorez le contenu de `fitridge`.

```{r,eval=F}
lambda_seq <- 10^(seq(-4,4,0.01))
fitridge <- glmnet(x = tildeX, y = tildeY, alpha = 0, lambda = lambda_seq,family=c("gaussian"),intercept=F) # A completer
summary(fitridge)
```

2.  Tracez les chemins de régularisation de chaque variable et
    commentez.

```{r,eval=F}
df=data.frame(lambda = rep(fitridge$lambda,ncol(tildeX)), theta=as.vector(t(fitridge$beta)),variable=rep(colnames(tildeX),each=length(fitridge$lambda)))
g1 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
ggplotly(g1)
```
Dans la zone des petit lambda, on est presque sur de la régression moindre carré classique.
Lorsque lambda augmente, les coefs des variables vont se contracté au fur et à mesure.

Remarque lorsque des variables sont tres corrélés les coefs de ses variables vont être ramener au même niveau.

Il nous faut donc choisir le lambda, pour cela on fait de la validation croisée.

On test la qualité des prédiction avec le MSE (somme des résidus) pour chaque lambda. Et on dissocie nos données en donnée de test et donnée pour le fit.


3.  A l'aide de la fonction `cv.glmnet()` mettez en place une validation
    croisée pour sélectionner le "meilleur" $\lambda$ par MSE.

```{r, eval=F}
ridge_cv <- cv.glmnet(x = tildeX, y = tildeY, alpha = 0, lambda = lambda_seq, nfolds=10, type.measure=c("mse"), intercept=F) # A completer
best_lambda <- ridge_cv$lambda.min
print(best_lambda)
```

```{r,eval=F}
df2=data.frame(lambda=ridge_cv$lambda,MSE=ridge_cv$cvm,cvup=ridge_cv$cvup,cvlo=ridge_cv$cvlo)
gmse<-ggplot(df2)+
  geom_line(aes(x=lambda,y=MSE))+
  geom_vline(xintercept = ridge_cv$lambda.min,col="red",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvup),col="blue",linetype="dotted")+
  geom_line(aes(x=lambda,y=cvlo),col="blue",linetype="dotted")+
  #xlim(c(0,ridge_cv$lambda.min+0.5))+
  scale_x_log10()
ggplotly(gmse)
```

La valeur de $\lambda$ sélectionnée vaut 0.1949845

```{r,eval=F}
g1=g1 + 
  geom_vline(xintercept = best_lambda,linetype="dotted", color = "red")+
  scale_x_log10()
```

## Régression Lasso

1.  A l'aide de la fonction `glmnet()`, ajustez une régression Lasso en
    faisant varier $\lambda$ sur une grille. On stockera le résultat
    dans la variable `fitlasso`. Explorez le contenu de `fitlasso`.

```{r,eval=F}
fitlasso <- glmnet(x = tildeX, y = tildeY, alpha = 1, lambda = lambda_seq, family=c("gaussian"), intercept=F) # A COMPLETER
summary(fitlasso)
```

2.  Tracez le chemin de régularisation de chacune des variables et
    commentez

```{r,eval=F}
df=data.frame(lambda = rep(fitlasso$lambda,ncol(tildeX)), theta=as.vector(t(fitlasso$beta)),variable=rep(colnames(tildeX),each=length(fitlasso$lambda)))
g3 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
ggplotly(g3)
```

3.  A l'aide de la fonction `cv.glmnet()` mettez en place une validation
    croisée pour sélectionner le "meilleur" $\lambda$ par MSE. En
    pratique, il est préconisé d'utilisé `lambda.1se` (la plus grande
    valeur de $\lambda$ telle que l'erreur standard se situe à moins de
    1 de celle du minimum).

```{r,eval=F}
lasso_cv <- cv.glmnet(x = tildeX, y = tildeY, alpha = 1, lambda = lambda_seq, nfolds=10, type.measure=c("mse"), intercept=F) # A COMPLETER
best_lambda <-lasso_cv$lambda.min
lambda1se <- lasso_cv$lambda.1se
print(lambda1se)
```

La valeur de $\lambda$ sélectionnée vaut 0.1862087

```{r,eval=F}
g3=g3 + 
  geom_vline(xintercept = best_lambda,linetype="dotted", color = "red")+
  geom_vline(xintercept = lambda1se,linetype="dotted", color = "blue")+
  scale_x_log10()
g3
```

4.  Quelle sélection de variables obtient-on alors ?

Vous pouvez utiliser la fonction `extract.coef()` de la librairie
`coefplot`

```{r}
extract.coef(lasso_cv, lambda = "lambda.min")
```
```{r}
extract.coef(lasso_cv, lambda = "lambda.1se")
```


## Régression Elastic Net

1.  Reprenez les questions précédentes pour ajuster une régression
    Elastic Net

```{r,eval=F}
fitEN <- glmnet(x = tildeX, y = tildeY, alpha = 0.5, lambda = lambda_seq, family=c("gaussian"),intercept=F)
df=data.frame(lambda = rep(fitEN$lambda,ncol(tildeX)), theta=as.vector(t(fitEN$beta)),variable=rep(c(colnames(tildeX)),each=length(fitEN$lambda)))
g4 = ggplot(df,aes(x=lambda,y=theta,col=variable))+
  geom_line()+
  theme(legend.position="bottom")+
  scale_x_log10()
```

```{r,eval=F}
EN_cv <- cv.glmnet(x = tildeX, y = tildeY, alpha = 0.5, lambda = lambda_seq, nfolds=10, type.measure=c("mse"), intercept=F)
best_lambda <-EN_cv$lambda.min
g4=g4 + geom_vline(xintercept = best_lambda,linetype="dotted", 
                color = "red")
ggplotly(g4)
```

2.  Comparez les coefficients obtenus avec la régression linéaire, la
    régression ridge, la régression Lasso et la régression ElasticNet

```{r ,eval=F,echo=F}
regusuel=lm(formula=maxO3~., data = ozone)  # A COMPLETER
df4=data.frame(x=rep(colnames(tildeX),4),
               coef=c(as.vector(regusuel$coefficients),as.vector(coef(ridge_cv,s=ridge_cv$lambda.min)[-1]),as.vector(coef(lasso_cv)[-1]),as.vector(coef(EN_cv)[-1])),
               reg=c(rep("reg.lin",ncol(tildeX)),rep("ridge",ncol(tildeX)),rep("lasso",ncol(tildeX)),rep("ElasticNet",ncol(tildeX))))

g5=ggplot(df4)+
  geom_point(aes(x=x,y=coef,col=reg))
g5
```
